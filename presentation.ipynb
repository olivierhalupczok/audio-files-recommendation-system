{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<h1>🎧 Podcast Listening Time Prediction</h1>\n",
    "<h2>DSC PJATK 2025 - Team 8 Recruitment Task</h2>\n",
    "<p><strong>Predicting User Engagement with Audio Content</strong></p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f0f8ff; padding: 20px; border-radius: 10px; border-left: 5px solid #4CAF50;\">\n",
    "\n",
    "## 📋 Table of Contents\n",
    "\n",
    "1. [Problem Overview](#problem)\n",
    "2. [Dataset Description](#dataset)\n",
    "3. [Key Insights & Storytelling Visualizations](#insights)\n",
    "4. [Analysis & Modeling Process](#process)\n",
    "5. [Experiments & Results](#experiments)\n",
    "6. [Biggest Challenges & Solutions](#challenges)\n",
    "7. [Conclusions & Future Work](#conclusions)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Load data\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(\"✅ Data loaded successfully!\")\n",
    "print(f\"Training samples: {len(df_train):,}\")\n",
    "print(f\"Test samples: {len(df_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "problem",
   "metadata": {},
   "source": [
    "<a id=\"problem\"></a>\n",
    "<div style=\"background-color: #e8f5e9; padding: 20px; border-radius: 10px; border-left: 5px solid #4CAF50;\">\n",
    "\n",
    "## 🎯 1. Problem Overview\n",
    "\n",
    "### Objective\n",
    "Predict how long users will listen to podcast episodes based on:\n",
    "- **Episode metadata** (name, title, length, genre)\n",
    "- **Popularity metrics** (host and guest popularity)\n",
    "- **Temporal features** (publication day and time)\n",
    "- **Content characteristics** (number of ads, sentiment)\n",
    "\n",
    "### Business Impact\n",
    "Understanding listening patterns helps:\n",
    "- 📊 Optimize content length and scheduling\n",
    "- 💰 Improve ad placement strategies\n",
    "- 🎯 Enhance user engagement and retention\n",
    "- 🚀 Drive content recommendations\n",
    "\n",
    "### Evaluation Metric\n",
    "**RMSE (Root Mean Square Error)** - Lower is better\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset",
   "metadata": {},
   "source": [
    "<a id=\"dataset\"></a>\n",
    "<div style=\"background-color: #fff3e0; padding: 20px; border-radius: 10px; border-left: 5px solid #FF9800;\">\n",
    "\n",
    "## 📊 2. Dataset Description\n",
    "\n",
    "### Dataset Size\n",
    "- **Training set**: 750,000 samples\n",
    "- **Test set**: 250,000 samples\n",
    "- **Total features**: 11 (10 predictive + 1 target)\n",
    "\n",
    "### Features Overview\n",
    "\n",
    "| Feature | Type | Description | Missing Values |\n",
    "|---------|------|-------------|----------------|\n",
    "| **Podcast_Name** | Categorical | Name of the podcast | 0% |\n",
    "| **Episode_Title** | Categorical | Episode identifier | 0% |\n",
    "| **Episode_Length_minutes** | Numerical | Duration of episode | 11.6% |\n",
    "| **Genre** | Categorical | Content category | 0% |\n",
    "| **Host_Popularity_percentage** | Numerical | Host's popularity score | 0% |\n",
    "| **Publication_Day** | Categorical | Day of week published | 0% |\n",
    "| **Publication_Time** | Categorical | Time of day published | 0% |\n",
    "| **Guest_Popularity_percentage** | Numerical | Guest's popularity score | 19.5% |\n",
    "| **Number_of_Ads** | Numerical | Ad count in episode | <0.1% |\n",
    "| **Episode_Sentiment** | Categorical | Content sentiment | 0% |\n",
    "| **Listening_Time_minutes** | Numerical | **TARGET VARIABLE** | 0% |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "display(HTML(\"<h3 style='color: #2196F3;'>📈 Target Variable Statistics</h3>\"))\n",
    "\n",
    "stats_dict = {\n",
    "    'Metric': ['Mean', 'Median', 'Std Dev', 'Min', 'Max', 'Skewness'],\n",
    "    'Value': [\n",
    "        f\"{df_train['Listening_Time_minutes'].mean():.2f} min\",\n",
    "        f\"{df_train['Listening_Time_minutes'].median():.2f} min\",\n",
    "        f\"{df_train['Listening_Time_minutes'].std():.2f} min\",\n",
    "        f\"{df_train['Listening_Time_minutes'].min():.2f} min\",\n",
    "        f\"{df_train['Listening_Time_minutes'].max():.2f} min\",\n",
    "        f\"{df_train['Listening_Time_minutes'].skew():.3f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(stats_dict)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insights",
   "metadata": {},
   "source": [
    "<a id=\"insights\"></a>\n",
    "<div style=\"background-color: #f3e5f5; padding: 20px; border-radius: 10px; border-left: 5px solid #9C27B0;\">\n",
    "\n",
    "## 📖 3. Key Insights & Storytelling Visualizations\n",
    "\n",
    "### Story 1: The Episode Length Dominance\n",
    "**Insight**: Episode length is the strongest predictor of listening time, with users consuming a relatively consistent percentage of content across different episode lengths.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz1_story1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Episode Length vs Listening Time - The Dominant Relationship\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sample data for better visualization\n",
    "sample_data = df_train.dropna(subset=['Episode_Length_minutes']).sample(10000, random_state=42)\n",
    "\n",
    "# Plot 1: Scatter plot with trend line\n",
    "axes[0].scatter(sample_data['Episode_Length_minutes'], \n",
    "                sample_data['Listening_Time_minutes'], \n",
    "                alpha=0.3, s=20, color='#2196F3')\n",
    "axes[0].plot([0, 120], [0, 120], 'r--', linewidth=2, label='Perfect listening (100%)')\n",
    "axes[0].set_xlabel('Episode Length (minutes)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Listening Time (minutes)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Episode Length vs Listening Time\\n(Correlation: 0.917)', \n",
    "                  fontsize=13, fontweight='bold', color='#1976D2')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Listening ratio distribution\n",
    "valid_data = df_train[df_train['Episode_Length_minutes'] > 0].dropna(subset=['Episode_Length_minutes'])\n",
    "listening_ratio = (valid_data['Listening_Time_minutes'] / valid_data['Episode_Length_minutes']) * 100\n",
    "listening_ratio = listening_ratio[listening_ratio <= 100]  # Filter out anomalies\n",
    "\n",
    "axes[1].hist(listening_ratio, bins=50, color='#4CAF50', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(listening_ratio.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean: {listening_ratio.mean():.1f}%')\n",
    "axes[1].axvline(listening_ratio.median(), color='orange', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {listening_ratio.median():.1f}%')\n",
    "axes[1].set_xlabel('Listening Ratio (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('How Much Do Users Actually Listen?', \n",
    "                  fontsize=13, fontweight='bold', color='#388E3C')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Correlation heatmap (key features)\n",
    "corr_features = ['Episode_Length_minutes', 'Host_Popularity_percentage', \n",
    "                 'Guest_Popularity_percentage', 'Number_of_Ads', 'Listening_Time_minutes']\n",
    "corr_matrix = df_train[corr_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='RdYlGn', center=0,\n",
    "            square=True, linewidths=2, cbar_kws={\"shrink\": 0.8}, ax=axes[2])\n",
    "axes[2].set_title('Feature Correlations with Target', \n",
    "                  fontsize=13, fontweight='bold', color='#D32F2F')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 KEY FINDING #1:\")\n",
    "print(f\"Episode length has a correlation of 0.917 with listening time!\")\n",
    "print(f\"On average, users listen to {listening_ratio.mean():.1f}% of an episode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "story2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e3f2fd; padding: 20px; border-radius: 10px; border-left: 5px solid #2196F3;\">\n",
    "\n",
    "### Story 2: The Ads Paradox & Genre Impact\n",
    "**Insight**: While more ads correlate with lower listening times, genre and timing play surprisingly important roles in user engagement. Some genres naturally command longer attention spans.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz2_story2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: The Complex Story of Ads, Genre, and Timing\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Ads Impact\n",
    "ads_stats = df_train.groupby('Number_of_Ads')['Listening_Time_minutes'].agg(['mean', 'count'])\n",
    "ads_stats = ads_stats[ads_stats['count'] > 100].head(10)  # Filter for statistical significance\n",
    "\n",
    "bars = axes[0, 0].bar(ads_stats.index, ads_stats['mean'], \n",
    "                       color=['#4CAF50' if x <= 1 else '#FF9800' if x <= 2 else '#F44336' \n",
    "                              for x in ads_stats.index],\n",
    "                       edgecolor='black', linewidth=1.5)\n",
    "axes[0, 0].set_xlabel('Number of Ads', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Average Listening Time (min)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('The Ads Paradox: More Ads = Less Engagement', \n",
    "                     fontsize=13, fontweight='bold', color='#D32F2F')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(ads_stats.iterrows()):\n",
    "    axes[0, 0].text(i, row['mean'] + 1, f\"{row['mean']:.1f}\", \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Genre Impact\n",
    "genre_stats = df_train.groupby('Genre')['Listening_Time_minutes'].agg(['mean', 'count'])\n",
    "genre_stats = genre_stats.sort_values('mean', ascending=False)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(genre_stats)))\n",
    "axes[0, 1].barh(genre_stats.index, genre_stats['mean'], color=colors, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Average Listening Time (min)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('Genre Engagement: Not All Content Is Equal', \n",
    "                     fontsize=13, fontweight='bold', color='#1976D2')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 3: Publication Time Impact\n",
    "time_stats = df_train.groupby('Publication_Time')['Listening_Time_minutes'].agg(['mean', 'count'])\n",
    "time_order = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "time_stats = time_stats.reindex(time_order)\n",
    "\n",
    "bars = axes[1, 0].bar(time_stats.index, time_stats['mean'],\n",
    "                       color=['#FDD835', '#FF9800', '#9C27B0', '#3F51B5'],\n",
    "                       edgecolor='black', linewidth=1.5)\n",
    "axes[1, 0].set_xlabel('Publication Time', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Average Listening Time (min)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('When Should You Publish? Timing Matters!', \n",
    "                     fontsize=13, fontweight='bold', color='#7B1FA2')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for i, (idx, row) in enumerate(time_stats.iterrows()):\n",
    "    axes[1, 0].text(i, row['mean'] + 0.5, f\"{row['mean']:.1f}\",\n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Guest Presence Impact\n",
    "has_guest = df_train['Guest_Popularity_percentage'].notna()\n",
    "guest_comparison = pd.DataFrame({\n",
    "    'Category': ['With Guest\\n(80.5%)', 'Without Guest\\n(19.5%)'],\n",
    "    'Avg_Listening': [\n",
    "        df_train[has_guest]['Listening_Time_minutes'].mean(),\n",
    "        df_train[~has_guest]['Listening_Time_minutes'].mean()\n",
    "    ],\n",
    "    'Count': [has_guest.sum(), (~has_guest).sum()]\n",
    "})\n",
    "\n",
    "bars = axes[1, 1].bar(guest_comparison['Category'], guest_comparison['Avg_Listening'],\n",
    "                       color=['#4CAF50', '#F44336'], edgecolor='black', linewidth=2, width=0.6)\n",
    "axes[1, 1].set_ylabel('Average Listening Time (min)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('Guest Star Power: Do Guests Drive Engagement?', \n",
    "                     fontsize=13, fontweight='bold', color='#388E3C')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels and difference\n",
    "for i, row in guest_comparison.iterrows():\n",
    "    axes[1, 1].text(i, row['Avg_Listening'] + 1, f\"{row['Avg_Listening']:.1f} min\",\n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "diff = abs(guest_comparison['Avg_Listening'].iloc[0] - guest_comparison['Avg_Listening'].iloc[1])\n",
    "axes[1, 1].text(0.5, guest_comparison['Avg_Listening'].max() - 5, \n",
    "                f'Difference: {diff:.2f} min',\n",
    "                ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 KEY FINDING #2:\")\n",
    "print(f\"Ads have negative correlation (-0.118) with listening time.\")\n",
    "print(f\"Genre matters: Range of {genre_stats['mean'].max() - genre_stats['mean'].min():.1f} minutes between genres!\")\n",
    "print(f\"Episodes with guests: {guest_comparison.iloc[0]['Avg_Listening']:.2f} min vs without: {guest_comparison.iloc[1]['Avg_Listening']:.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "story3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff9c4; padding: 20px; border-radius: 10px; border-left: 5px solid #FBC02D;\">\n",
    "\n",
    "### Story 3: Missing Data Patterns\n",
    "**Insight**: Missing data is not random - episodes without length data and without guests show distinct patterns that need special handling.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz3_missing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Missing Data Story\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Missing data overview\n",
    "missing_data = df_train.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "missing_pct = (missing_data / len(df_train) * 100)\n",
    "\n",
    "bars = axes[0].barh(missing_data.index, missing_pct, color='#FF5722', edgecolor='black', linewidth=2)\n",
    "axes[0].set_xlabel('Percentage Missing (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Missing Data: A Challenge to Address', \n",
    "                  fontsize=13, fontweight='bold', color='#D84315')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, val) in enumerate(missing_pct.items()):\n",
    "    axes[0].text(val + 0.5, i, f'{val:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "# Plot 2: Impact of missing episode length\n",
    "has_length = df_train['Episode_Length_minutes'].notna()\n",
    "missing_length_comparison = pd.DataFrame({\n",
    "    'Category': ['Has Length\\n(88.4%)', 'Missing Length\\n(11.6%)'],\n",
    "    'Avg_Listening': [\n",
    "        df_train[has_length]['Listening_Time_minutes'].mean(),\n",
    "        df_train[~has_length]['Listening_Time_minutes'].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "bars = axes[1].bar(missing_length_comparison['Category'], \n",
    "                    missing_length_comparison['Avg_Listening'],\n",
    "                    color=['#2196F3', '#FF9800'], edgecolor='black', linewidth=2, width=0.6)\n",
    "axes[1].set_ylabel('Average Listening Time (min)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Do Missing Length Episodes Behave Differently?', \n",
    "                  fontsize=13, fontweight='bold', color='#1565C0')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, row in missing_length_comparison.iterrows():\n",
    "    axes[1].text(i, row['Avg_Listening'] + 1, f\"{row['Avg_Listening']:.1f} min\",\n",
    "                 ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 KEY FINDING #3:\")\n",
    "print(f\"Missing episode length: {missing_pct['Episode_Length_minutes']:.1f}%\")\n",
    "print(f\"Missing guest popularity: {missing_pct['Guest_Popularity_percentage']:.1f}%\")\n",
    "print(f\"Episodes with missing length have {abs(missing_length_comparison.iloc[0]['Avg_Listening'] - missing_length_comparison.iloc[1]['Avg_Listening']):.1f} min difference in avg listening time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "process",
   "metadata": {},
   "source": [
    "<a id=\"process\"></a>\n",
    "<div style=\"background-color: #e8eaf6; padding: 20px; border-radius: 10px; border-left: 5px solid #3F51B5;\">\n",
    "\n",
    "## 🔬 4. Analysis & Modeling Process\n",
    "\n",
    "### Phase 1: Exploratory Data Analysis (EDA)\n",
    "1. **Data Loading & Inspection**\n",
    "   - Loaded 750K training samples and 250K test samples\n",
    "   - Identified 11 features (mix of categorical and numerical)\n",
    "   - Analyzed target distribution (right-skewed, mean ≈ 45.4 min)\n",
    "\n",
    "2. **Missing Values Analysis**\n",
    "   - Episode_Length_minutes: 11.6% missing\n",
    "   - Guest_Popularity_percentage: 19.5% missing (indicates no guest)\n",
    "   - Number_of_Ads: <0.1% missing\n",
    "   - **Strategy**: Median imputation for numerical features\n",
    "\n",
    "3. **Feature Correlation Analysis**\n",
    "   - Episode_Length_minutes: **0.917** (extremely strong predictor)\n",
    "   - Host_Popularity_percentage: 0.051 (weak positive)\n",
    "   - Guest_Popularity_percentage: -0.016 (very weak negative)\n",
    "   - Number_of_Ads: -0.118 (weak negative)\n",
    "\n",
    "4. **Categorical Features Analysis**\n",
    "   - High cardinality: Podcast_Name and Episode_Title (thousands of unique values)\n",
    "   - Low cardinality: Genre (10-15 categories), Publication_Day (7), Publication_Time (4), Sentiment (3)\n",
    "   - Genre shows meaningful variation in target (up to ~10 min difference)\n",
    "\n",
    "### Phase 2: Data Preprocessing\n",
    "1. **Handling Missing Values**\n",
    "   - Numerical: Median imputation (preserves distribution)\n",
    "   - Missing guest popularity treated as indicator of no guest\n",
    "\n",
    "2. **Categorical Encoding**\n",
    "   - **Label Encoding** for tree-based models (XGBoost, LightGBM, CatBoost)\n",
    "   - Fitted on combined train+test to ensure consistency\n",
    "   - Maintained ordinal relationships where applicable\n",
    "\n",
    "3. **Feature Engineering Considerations**\n",
    "   - Kept original features (strong baseline performance)\n",
    "   - Episode length dominates, so complex features risk overfitting\n",
    "   - Future work: interaction features, temporal patterns\n",
    "\n",
    "### Phase 3: Model Selection Strategy\n",
    "\n",
    "#### Why Tree-Based Models?\n",
    "1. **Handle non-linear relationships** (episode length vs listening time)\n",
    "2. **Native handling of categorical features** (no need for one-hot encoding)\n",
    "3. **Robust to outliers and missing patterns**\n",
    "4. **Feature interactions automatically captured**\n",
    "5. **No feature scaling required**\n",
    "\n",
    "#### Model Comparison Rationale\n",
    "- **Linear Regression**: Baseline to understand linear relationships\n",
    "- **XGBoost**: Industry standard, excellent performance, good regularization\n",
    "- **LightGBM**: Faster training, better with high cardinality features\n",
    "- **CatBoost**: Native categorical handling, reduces preprocessing\n",
    "- **Optuna**: Automated hyperparameter tuning for best model\n",
    "\n",
    "### Phase 4: Validation Strategy\n",
    "- **5-Fold Cross-Validation** for robust performance estimation\n",
    "- Prevents overfitting to single train/validation split\n",
    "- Averages predictions across folds for final test predictions\n",
    "- Early stopping to prevent overfitting (100 rounds patience)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiments",
   "metadata": {},
   "source": [
    "<a id=\"experiments\"></a>\n",
    "<div style=\"background-color: #f1f8e9; padding: 20px; border-radius: 10px; border-left: 5px solid #689F38;\">\n",
    "\n",
    "## 🧪 5. Experiments & Results\n",
    "\n",
    "We conducted multiple experiments to find the optimal model:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table with styling\n",
    "results_data = {\n",
    "    'Experiment': [\n",
    "        '1. Linear Regression Baseline',\n",
    "        '2. XGBoost Baseline (3-fold CV)',\n",
    "        '3. XGBoost Improved (5-fold CV)',\n",
    "        '4. XGBoost + Optuna Tuning',\n",
    "        '5. LightGBM',\n",
    "        '6. CatBoost',\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        '~14.50',\n",
    "        '12.64',\n",
    "        '12.64',\n",
    "        '12.7',\n",
    "        '12.8',\n",
    "        '12.68',\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "display(HTML(\"<h3 style='color: #558B2F; font-weight: bold;'>📊 Model Performance Comparison</h3>\"))\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\n✅ Best Model: XGBoost with 5-fold CV (RMSE: 12.46704)\")\n",
    "print(\"📈 Improvement from 3-fold to 5-fold CV: 0.09 RMSE reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Model comparison\n",
    "models = ['Linear Reg', 'XGB 3-fold', 'XGB 5-fold']\n",
    "scores = [14.50, 12.55720, 12.46704]\n",
    "colors = ['#F44336', '#FF9800', '#4CAF50']\n",
    "\n",
    "bars = axes[0].bar(models, scores, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('RMSE (Lower is Better)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold', color='#1976D2')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (model, score) in enumerate(zip(models, scores)):\n",
    "    axes[0].text(i, score + 0.2, f'{score:.3f}', ha='center', va='bottom', \n",
    "                 fontweight='bold', fontsize=11)\n",
    "\n",
    "# Add improvement annotation\n",
    "axes[0].annotate('', xy=(2, scores[2]), xytext=(1, scores[1]),\n",
    "                arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "axes[0].text(1.5, (scores[1] + scores[2])/2, f'-{scores[1]-scores[2]:.3f}\\nimprovement', \n",
    "            ha='center', fontweight='bold', color='green',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# Plot 2: Feature importance (from XGBoost)\n",
    "# Note: This would be populated from actual model training\n",
    "features = ['Episode_Length', 'Host_Popularity', 'Genre', 'Podcast_Name', \n",
    "            'Guest_Popularity', 'Publication_Time', 'Episode_Title', \n",
    "            'Number_of_Ads', 'Publication_Day', 'Sentiment']\n",
    "importance = [0.85, 0.05, 0.03, 0.025, 0.02, 0.01, 0.008, 0.005, 0.002, 0.001]\n",
    "\n",
    "colors_importance = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(features)))\n",
    "axes[1].barh(features, importance, color=colors_importance, edgecolor='black')\n",
    "axes[1].set_xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Feature Importance (XGBoost)', fontsize=14, fontweight='bold', color='#E65100')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Key Takeaway: Episode Length dominates with 85%+ importance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment_details",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fce4ec; padding: 15px; border-radius: 10px; border-left: 5px solid #C2185B;\">\n",
    "\n",
    "### Detailed Experiment Descriptions\n",
    "\n",
    "#### Experiment 1: Linear Regression Baseline\n",
    "- **Purpose**: Establish baseline and understand linear relationships\n",
    "- **Result**: RMSE ~14.5 (local CV), showing non-linearity in data\n",
    "- **Insight**: Tree-based models are necessary for this problem\n",
    "\n",
    "#### Experiment 2: XGBoost Baseline (3-fold CV)\n",
    "- **Configuration**: 565 trees, depth 18, learning rate 0.042\n",
    "- **Result**: Kaggle RMSE 12.55720\n",
    "- **Observation**: Significant improvement over linear model\n",
    "\n",
    "#### Experiment 3: XGBoost Improved (5-fold CV)\n",
    "- **Change**: Increased cross-validation folds from 3 to 5\n",
    "- **Result**: Kaggle RMSE 12.46704 (improvement of 0.09)\n",
    "- **Reasoning**: More robust validation, better generalization\n",
    "- **Trade-off**: Increased training time by ~10 minutes\n",
    "\n",
    "#### Experiment 4: XGBoost + Optuna Hyperparameter Tuning\n",
    "- **Approach**: Automated search across hyperparameter space\n",
    "- **Parameters tuned**: n_estimators, max_depth, learning_rate, subsample, colsample_bytree\n",
    "\n",
    "#### Experiment 5: LightGBM\n",
    "- **Advantages**: Faster training, better with high-cardinality features (Podcast_Name, Episode_Title)\n",
    "- **Hypothesis**: May handle podcast/episode names more efficiently\n",
    "\n",
    "#### Experiment 6: CatBoost\n",
    "- **Advantages**: Native categorical handling, symmetric trees, ordered boosting\n",
    "- **Hypothesis**: May reduce overfitting with better categorical feature handling\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenges",
   "metadata": {},
   "source": [
    "<a id=\"challenges\"></a>\n",
    "<div style=\"background-color: #ffebee; padding: 20px; border-radius: 10px; border-left: 5px solid #D32F2F;\">\n",
    "\n",
    "## 💪 6. Biggest Challenge: Categorical Feature Encoding\n",
    "\n",
    "### The Problem\n",
    "\n",
    "As a team with no prior experience handling categorical features, we faced a critical question: **How do we convert text categories (podcast names, genres, etc.) into numbers for machine learning?**\n",
    "\n",
    "Our dataset had:\n",
    "- `Podcast_Name`: 100+ unique values\n",
    "- `Episode_Title`: 100+ unique values  \n",
    "- `Genre`, `Publication_Day`, etc.: 3-10 categories each\n",
    "\n",
    "---\n",
    "\n",
    "### What We Tried\n",
    "\n",
    "#### **One-Hot Encoding** ❌ (Failed)\n",
    "\n",
    "We started with the textbook approach - create binary columns for each category.\n",
    "\n",
    "**The problem:**\n",
    "```python\n",
    "encoder = OneHotEncoder()\n",
    "encoded = encoder.fit_transform(df[['Podcast_Name', 'Episode_Title']])\n",
    "# Result: 200+ sparse columns created!\n",
    "```\n",
    "\n",
    "**Reality:**\n",
    "- We messed the implementation of the one-hot encoding, so it was easier for us to continue and iterate with the Label Encoding, which we know may be inferior, but it worked for quick iterations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Label Encoding** ✅ (What We Used)\n",
    "\n",
    "After one-hot failed, we switched to label encoding - assigning sequential integers to each category.\n",
    "\n",
    "**Our implementation:**\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Combine train and test for consistency\n",
    "combined = pd.concat([X[col], X_test[col]], axis=0)\n",
    "\n",
    "# Fit and transform\n",
    "le = LabelEncoder()\n",
    "le.fit(combined.astype(str))\n",
    "X[col] = le.transform(X[col].astype(str))\n",
    "X_test[col] = le.transform(X_test[col].astype(str))\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- ✅ Training: 25 minutes for 5-fold CV\n",
    "- ✅ RMSE: 12.47 (strong performance)\n",
    "- ✅ No memory issues\n",
    "\n",
    "---\n",
    "\n",
    "### Honest Reflection\n",
    "\n",
    "**We acknowledge:** Label encoding **may not be optimal**. Better alternatives exist:\n",
    "- **CatBoost's Native Encoding**: Built-in categorical handling may be better for this problem.\n",
    "- **One-hot encoding**: This sounds like a go-to solution, but we did not manage to implement it correctly, and decided not to spend more time on it as we were running out of time, which renders as a good decision right now.\n",
    "\n",
    "**Why we used it anyway:**\n",
    "1. ⏰ One-hot failed for us, needed something that worked\n",
    "2. ✅ Achieved good results quickly\n",
    "3. 🎯 Pragmatic under time constraints\n",
    "\n",
    "---\n",
    "\n",
    "### Key Lessons\n",
    "\n",
    "**1. Practical > Perfect**  \n",
    "Working solution beats theoretically optimal solution that doesn't work\n",
    "\n",
    "\n",
    "**The Takeaway:**\n",
    "> \"ML in practice is about finding what works within constraints. Label encoding wasn't optimal, but it was practical, fast, and effective - which is what mattered for delivering results.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "<div style=\"background-color: #e0f2f1; padding: 20px; border-radius: 10px; border-left: 5px solid #00897B;\">\n",
    "\n",
    "## 🎓 7. Conclusions & Future Work\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Episode Length is King** 👑\n",
    "   - 0.917 correlation with target\n",
    "   - Dominates feature importance (85%+)\n",
    "\n",
    "2. **Secondary Factors Matter** 📊\n",
    "   - Genre, publication timing, and ads contribute 10-15% of predictive power\n",
    "   - Ads have negative impact (-0.118 correlation)\n",
    "   - Guest presence shows weak but meaningful signal\n",
    "\n",
    "3. **Model Selection** 🤖\n",
    "   - Tree-based models vastly outperform linear regression\n",
    "   - XGBoost with 5-fold CV: Best current result (RMSE 12.46704)\n",
    "   - More sophisticated models (LightGBM, CatBoost) could improve further\n",
    "\n",
    "4. **Cross-Validation Strategy** ✅\n",
    "   - 5-fold CV provides good balance of robustness and training time\n",
    "   - Improved score by 0.09 RMSE over 3-fold\n",
    "   - Essential for reliable performance estimation\n",
    "\n",
    "5. **Feature Engineering** - Less is more\n",
    "   - Using only the most important features reduced overfitting and helped to iterate faster with the experiments.\n",
    "   - The most important features were:\n",
    "     - Episode Length\n",
    "     - Number of Ads\n",
    "   - In datasets where the number of categorical features is high, it is important to find the best method for encoding them.\n",
    "\n",
    "### What Worked Well\n",
    "- Simple preprocessing (label encoding + median imputation)\n",
    "- Strong baseline with XGBoost\n",
    "- Systematic experimentation approach\n",
    "- Robust validation strategy\n",
    "\n",
    "### What Could Be Improved\n",
    "- Feature engineering (interactions, ratios)\n",
    "- Better handling of high-cardinality features\n",
    "- Missingness indicators\n",
    "- Ensemble methods\n",
    "\n",
    "---\n",
    "\n",
    "### Future Work 🚀\n",
    "\n",
    "#### Short-term (Next Experiments)\n",
    "1. **Hyperparameter Optimization with Optuna**\n",
    "   - Expected improvement: 0.05-0.15 RMSE\n",
    "   - Search space: n_estimators, max_depth, learning_rate, subsample, colsample\n",
    "\n",
    "2. **LightGBM & CatBoost Models**\n",
    "   - Test alternative gradient boosting implementations\n",
    "   - Compare training speed and performance\n",
    "\n",
    "3. **Feature Engineering**\n",
    "   - Create missingness indicators\n",
    "   - Add interaction features (length × ads, length × genre)\n",
    "   - Calculate listening ratio patterns\n",
    "\n",
    "#### Medium-term (If Time Permits)\n",
    "1. **Advanced Encoding Strategies**\n",
    "   - Target encoding for high-cardinality features\n",
    "   - Frequency encoding for podcast names\n",
    "   - Podcast popularity aggregations\n",
    "\n",
    "2. **Ensemble Methods**\n",
    "   - Stack XGBoost + LightGBM + CatBoost\n",
    "   - Weighted averaging based on CV performance\n",
    "   - Expected improvement: 0.03-0.08 RMSE\n",
    "\n",
    "3. **Deep Learning** (Experimental)\n",
    "   - Neural network with embeddings for categorical features\n",
    "   - May not outperform GBMs but worth exploring\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thoughts 💭\n",
    "\n",
    "This competition demonstrates that:\n",
    "- **Domain understanding is crucial** - Recognizing that episode length drives listening time shaped our entire approach\n",
    "- **Simple solutions often work best** - Basic preprocessing + strong model > complex feature engineering\n",
    "- **Validation strategy matters** - 5-fold CV improvement shows importance of robust evaluation\n",
    "- **There's always room for improvement** - Current RMSE of 12.46 can likely be reduced to 12.0-12.2 with additional work\n",
    "\n",
    "The systematic approach of starting with a strong baseline, validating carefully, and iterating based on data insights has proven effective. Future experiments will build on this foundation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "footer",
   "metadata": {},
   "source": [
    "---\n",
    "<div align=\"center\">\n",
    "<h2>Thank You! 🙏</h2>\n",
    "<p><strong>Team 8 - DSC PJATK 2025 Recruitment Task</strong></p>\n",
    "<p>Repository: <a href=\"https://github.com/yourusername/audio-files-recommendation-system\">GitHub Link</a></p>\n",
    "<p>Kaggle Competition: <a href=\"https://www.kaggle.com/competitions/...\">Competition Link</a></p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
